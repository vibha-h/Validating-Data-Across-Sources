{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13f5c47-dc8b-48b7-b56c-47508a246548",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "## DATA VALIDATION\n_Vibha Hodachalli_\n\nThis notebook details the process of validating data from two data marts, one in Oracle and the other in Snowflake. \n"
  },
  {
   "cell_type": "markdown",
   "id": "891a662b-8585-4e01-b2d2-6128f2e50f1e",
   "metadata": {
    "name": "query_step_1",
    "collapsed": false
   },
   "source": "## 1. Get subset from Snowflake and Oracle SQL Dev  \n\n- The primary key can be used to sort the data.  \n     - primary key is specified in the columns tab of Oracle SQL Dev when a table is selected."
  },
  {
   "cell_type": "code",
   "id": "cf1e7bbb-adf1-4fe4-90ee-65943ce29be4",
   "metadata": {
    "language": "sql",
    "name": "snowflake_query",
    "collapsed": false
   },
   "outputs": [],
   "source": "//Role must be set to access the marts\nUSE ROLE ADMIN;\n\n//It may be a good idea to skim over the data and see what subset of it you want to use\nSELECT * FROM SNOWFLAKE_MART ORDER BY PRIMARY_KEY ASC;\n\n//I observed that the DATE_COL and ... dates range from 2010 to 2024, I decided to take a subset where the ... was b/w 2020-2021. Ordered by PRIMARY_KEY (which is the primary key) ascending\nSELECT * FROM SNOWFLAKE_MART WHERE YEAR(DATE_COL) BETWEEN 2020 AND 2021 ORDER BY PRIMARY_KEY ASC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f245ab7a-3d14-4d0f-b3ed-3f5ce2856285",
   "metadata": {
    "language": "sql",
    "name": "oracle_query"
   },
   "outputs": [],
   "source": "//SQL query to get subset from Oracle SQL Dev\nSELECT \nCOL1,\nTO_CHAR(DATE_COL, 'YYYY-MM-DD HH24:MI:SS\".000 Z\"') AS COL2,\nCOL3,\nPRIMARY_KEY\nFROM ORACLE_MART WHERE EXTRACT(YEAR FROM DATE_COL) BETWEEN 2020 AND 2021 ORDER BY PRIMARY_KEY ASC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e83bb9b4-697e-471f-9954-f1351c4e8a8b",
   "metadata": {
    "name": "create_tables_step_2",
    "collapsed": false
   },
   "source": "## 2. Create Tables\n\n- You can create tables in Snowflake from files by clicking the + button in the far left upper corner\n\n- From the two queries above, I exported the results to csv files and created two tables in VIBHA_TEST.TEST  \n1. ORACLE_TABLE from the oracle query\n2. SNOWFLAKE_TABLE from the snowflake query\n\n- Make sure that the data types for each column are the same when creating the tables.  \n    - _You can just change all data types to VARCHAR for simplicity_"
  },
  {
   "cell_type": "markdown",
   "id": "65baaa7e-a861-4172-8b55-b0acc218a6d2",
   "metadata": {
    "name": "update_step_3",
    "collapsed": false
   },
   "source": "## 3. Observe and Update Tables  \nQuery the tables and look over the data.   \nMake any updates if necessary.\n\n_For example:_  \n- I observed that some columns in the ORACLE_TABLE table had empty values where the SNOWFLAKE_TABLE table had NULL values.\n"
  },
  {
   "cell_type": "code",
   "id": "c5af964b-24ea-403d-96fa-bb83287329ab",
   "metadata": {
    "language": "sql",
    "name": "update_query",
    "collapsed": false
   },
   "outputs": [],
   "source": "USE ROLE VIBHA;\n\nSELECT * FROM VIBHA.SNOWFLAKE_TABLE WHERE COL1 IS NULL; //This query produced results\n\nSELECT * FROM VIBHA.ORACLE_TABLE WHERE COL1 IS NULL; //This query did not\n\nSELECT * FROM VIBHA.ORACLE_TABLE WHERE COL1 = ''; //Here I realized that the \"null\" values were being displayed as empty strings\n\n//To fix this issue, I used the following queries to update the table values in VIBHA.ORACLE_TABLE:\nUPDATE VIBHA.ORACLE_TABLE\nSET COL1 = CASE WHEN COL1 = '' THEN NULL ELSE COL1 END;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f28e3a15-17a5-4499-9e7a-afe6efe18883",
   "metadata": {
    "name": "run_script_step_4",
    "collapsed": false
   },
   "source": "## 4. Run below Python script using appropriate query in lines 8 and 9\n- If any differences are found, you will be prompted with a button to download a csv of the rows that differ"
  },
  {
   "cell_type": "code",
   "id": "7fe44006-a20a-48bb-af8c-140ef7909da5",
   "metadata": {
    "language": "python",
    "name": "py_comparison_script",
    "collapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark.context import get_active_session\nimport streamlit as st\n\n# Get active Snowflake session\nsession = get_active_session()\n\n# Fetch data from Snowflake and Oracle into dataframes\noracle_df = session.sql('SELECT * FROM VIBHA.ORACLE_TABLE LIMIT 10000')\nsnowflake_df = session.sql('SELECT * FROM VIBHA.SNOWFLAKE_TABLE LIMIT 10000')\n\nprint(\"oracle_df: \")\noracle_df.show()\n\nprint(\"snowflake_df: \")\nsnowflake_df.show()\n\n# Convert Snowflake and Oracle dataframes to pandas dataframes\nsnowflake_pandas_df = snowflake_df.to_pandas()\noracle_pandas_df = oracle_df.to_pandas()\n\n# Add a new column \"Table\" whose value is Oracle/Snowflake respectively\noracle_pandas_df.insert(loc=0, column='Table', value='Oracle')\nsnowflake_pandas_df.insert(loc=0, column='Table', value='Snowflake')\n\n# Compare dataframes excluding the 'Table' column\ndiff_df = oracle_pandas_df.loc[:, oracle_pandas_df.columns != 'Table'].compare(snowflake_pandas_df.loc[:, snowflake_pandas_df.columns != 'Table'], align_axis=0, keep_equal=False)\nprint(\"Results from comparison:\\n\",diff_df)\n\n# Get indices where there are differences\ndiff_indices = diff_df.index.get_level_values(0).unique()\n\n# Initialize an empty list to store dataframes to concatenate\nconcat_dataframes = []\n\n# Loop through the different indices\nfor idx in diff_indices:\n    # Append the row from Oracle dataframe to concat_dataframes list\n    concat_dataframes.append(oracle_pandas_df.iloc[[idx]])\n\n    # Append the row from Snowflake dataframe to concat_dataframes list\n    concat_dataframes.append(snowflake_pandas_df.iloc[[idx]])\nif concat_dataframes:\n   # Concatenate all dataframes in concat_dataframes list into result_df\n   result_df = pd.concat(concat_dataframes, ignore_index=True)\n\n    # Now result_df contains all rows from Oracle and Snowflake where differences were found\n   print(\"All column and row values for the differences:\\n\",result_df)\n    \n   #Download the dataframe as a .csv file\n   @st.cache_data\n   def convert_df(df):\n    return df.to_csv(index=False).encode('utf-8')\n\n\n   csv = convert_df(result_df)\n\n   st.download_button(\n   \"Press to Download CSV\",\n   csv,\n   \"file.csv\",\n   \"text/csv\",\n   key='download-csv'\n   )\nelse:\n    print(\"No differences found!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "08d37f86-b4ed-44f9-aa48-b5acd1dbc44f",
   "metadata": {
    "name": "comparison_results",
    "collapsed": false
   },
   "source": "The results from the above comparison should return an empty data frame, meaning that no differences were found!\n\nThe data can always be further validated though ->"
  },
  {
   "cell_type": "markdown",
   "id": "045b81e2-c7b8-46d9-9755-e7f6c52f5558",
   "metadata": {
    "name": "further_validation_step_5",
    "collapsed": false
   },
   "source": "## 5. Analyze and Validate Further\n- If differences are found,\n    - Analyze sources of differences by looking at the downloadable .csv file, SQL stored procedures used to create the marts, and source data in cerner millenium\n- Validate the next set of rows to see if any new differences come up"
  },
  {
   "cell_type": "code",
   "id": "d2a4dee5-97dc-4258-b626-cbfb47bb9e3e",
   "metadata": {
    "language": "sql",
    "name": "SQL_further_validation",
    "collapsed": false
   },
   "outputs": [],
   "source": "// In the first run of the script, 10,000 rows each from the Oracle and Snowflake marts were compared\n// Next we might compare the following 10,000 rows\n// In order to be compare the next 10,000 and not repeat any rows from the initial validation, the following query can be used:\nSELECT * \nFROM VIBHA.ORACLE_TABLE \nWHERE PRIMARY_KEY \n// The following conditional is the original query used in the initial validation run\nNOT IN (SELECT PRIMARY_KEY FROM VIBHA.ORACLE_TABLE ORDER BY PRIMARY_KEY ASC LIMIT 10000) ORDER BY PRIMARY_KEY ASC LIMIT 10000;\n\nSELECT * \nFROM VIBHA.SNOWFLAKE_TABLE \nWHERE PRIMARY_KEY \n// The following conditional is the original query used in the initial validation run\nNOT IN (SELECT PRIMARY_KEY FROM VIBHA.SNOWFLAKE_TABLE  ORDER BY PRIMARY_KEY ASC LIMIT 10000) ORDER BY PRIMARY_KEY ASC LIMIT 10000;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a1703ab-612e-4b8b-a8e5-893d77265b0f",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Further Validation (cont)\n- The same Python script as used in the initial validation can be used again\n    - Replace the query in lines 8 and 9 with the query developed above as to not repeat the validation of any rows"
  },
  {
   "cell_type": "code",
   "id": "d1ef7c65-7fc4-4395-98bf-d8d9107abc1a",
   "metadata": {
    "language": "python",
    "name": "py_further_validation",
    "collapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark.context import get_active_session\nimport streamlit as st\n\n# Get active Snowflake session\nsession = get_active_session()\n\n# Fetch data from Snowflake and Oracle into dataframes\n# session.sql('YOUR QUERY GOES HERE')\noracle_df = session.sql('SELECT * FROM VIBHA.ORACLE_TABLE  WHERE PRIMARY_KEY NOT IN (SELECT PRIMARY_KEY FROM VIBHA.ORACLE_TABLE ORDER BY PRIMARY_KEY ASC LIMIT 10000) ORDER BY PRIMARY_KEY ASC LIMIT 10000')\nsnowflake_df = session.sql('SELECT * FROM VIBHA.SNOWFLAKE_TABLE WHERE PRIMARY_KEY NOT IN (SELECT PRIMARY_KEY FROM VIBHA.SNOWFLAKE_TABLE ORDER BY PRIMARY_KEY ASC LIMIT 10000) ORDER BY PRIMARY_KEY ASC LIMIT 10000')\n\nprint(\"oracle_df: \")\noracle_df.show()\n\nprint(\"snowflake_df: \")\nsnowflake_df.show()\n\n# Convert Snowflake and Oracle dataframes to pandas dataframes\nsnowflake_pandas_df = snowflake_df.to_pandas()\noracle_pandas_df = oracle_df.to_pandas()\n\n# Add a new column \"Table\" whose value is Oracle/Snowflake respectively\noracle_pandas_df.insert(loc=0, column='Table', value='Oracle')\nsnowflake_pandas_df.insert(loc=0, column='Table', value='Snowflake')\n\n# Compare dataframes excluding the 'Table' column\ndiff_df = oracle_pandas_df.loc[:, oracle_pandas_df.columns != 'Table'].compare(snowflake_pandas_df.loc[:, snowflake_pandas_df.columns != 'Table'], align_axis=0, keep_equal=True)\nprint(\"Results from comparison:\\n\",diff_df)\n\n# Get indices where there are differences\ndiff_indices = diff_df.index.get_level_values(0).unique()\n\n# Initialize an empty list to store dataframes to concatenate\nconcat_dataframes = []\n\n# Loop through the different indices\nfor idx in diff_indices:\n    # Append the row from Oracle dataframe to concat_dataframes list\n    concat_dataframes.append(oracle_pandas_df.iloc[[idx]])\n\n    # Append the row from Snowflake dataframe to concat_dataframes list\n    concat_dataframes.append(snowflake_pandas_df.iloc[[idx]])\nif concat_dataframes:\n   # Concatenate all dataframes in concat_dataframes list into result_df\n   result_df = pd.concat(concat_dataframes, ignore_index=True)\n\n    # Now result_df contains all rows from Oracle and Snowflake where differences were found\n   print(\"All column and row values for the differences:\\n\",result_df)\n   \n    #Download the dataframe as a .csv file\n   @st.cache_data\n   def convert_df(df):\n    return df.to_csv(index=False).encode('utf-8')\n\n\n   csv = convert_df(result_df)\n\n   st.download_button(\n   \"Press to Download CSV\",\n   csv,\n   \"file.csv\",\n   \"text/csv\",\n   key='download-csv'\n   )\nelse:\n    print(\"No differences found!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8a071bd-0aac-4cf0-9502-1963bfa75301",
   "metadata": {
    "name": "conclusion",
    "collapsed": false
   },
   "source": "## That's all!\nYou can always validate even further and keep repeating step 5, while updating the query used :)"
  }
 ]
}